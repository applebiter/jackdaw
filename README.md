# jackdaw
So far, this project has just been a sandbox to rough in some ideas. The loop in
main.py waits for an input audio (a person querying the language model), and 
then when one is supplied, it is sent to OpenAI's whisper model to generate a 
transcription and the input audio is deleted. 

The transcription is saved as a text file and is soon discovered by the loop, 
which then sends the transcription to a language model to generate a response. 

The response is then received and saved to a new input text file as the 
transcription file is deleted. 

The new text file, representing the language model's response, is then 
discovered and submitted to MaryTTS, running on another host. 

The audio file generated by MaryTTS is then sent back to the original server but 
is not yet suitable for playback. At this point, the sox program is invoked from 
the command line to convert the audio to stereo, 32-bit, 44.1k WAV format (to 
match the default settings on my own system) and the original audio is deleted, 
along with the language model's text response. 

The final audio output, representing the language model's voice response to the 
original query, is then played on the JACK bus with a JACK client that is 
instantiated by the loop in main.py. When the playback is finished, the JACK 
client melts away and the audio file is deleted. The loop then waits for the 
next query.

### Update
This is now a graphical application. It sits in the system tray.

### Requirements
Linux -- This code is written for Linux. It may work on other operating systems,
but it has not been tested on them. 

JACK2 -- The JACK server must be running on the host machine. 

JackTrip -- JackTrip is optional and not used directly in this code, but it is used to 
connect all of the PCs on the network already running JACK. High-quality, multi-
channel audio can be sent selectively between the PCs on the network with very 
low latency. PCs connected with JackTrip and running JACK are able to process 
audio in real-time.

MaryTTS -- MaryTTS sits in that weird place that is not entirely unpleasant, but 
definitely does not sound like a human. I don't know whether anyone would want 
to hear an entire audiobook in that voice, but it suits as a voice for the house 
computer everywhere all at once on the JACK bus, running on connected PCs.

OpenAI Whisper -- OpenAI's Whisper does such a great job of transcribing 
punctuation that MaryTTS is really given it's best chance to shine. 

SOX -- SOX is used to convert the audio file from MaryTTS to a format that JACK
can play.

Ollama -- The Ollama server makes all of the Language Model magic happen. The 
Ollama server is a RESTful API that can be queried with text and will return
text. The Ollama server is not included in this project, but is central to it.

### Installation
1. Install JACK2, MaryTTS, SOX, and Ollama.
2. Clone this repository.
3. Create a virtual environment (`python3 -m venv .venv`).
4. Activate the virtual environment (`source .venv/bin/activate`).
5. Run `pip install -r requirements.txt` in the project directory.
6. Run `python main.py` in the project directory.
7. The application will appear in the system tray.
8. Right-click the icon to open the menu.
9. Select "Quit" to close the application
10. A `jackdaw.desktop` file example is included (desktop launcher) in the `/scripts` folder. Optional.

### Usage
1. Right-click the icon in the system tray to open the menu.
2. Click "Start Recording" to begin recording audio.
3. Speak into the microphone.
4. Click "Stop Recording" to stop recording audio.

The application will detect the audio and begin processing it. Also, this little
app is kind of a memory beast. No leaks, but when the libs are all loaded it 
comes in at nearly 1GB. The CPU usage is negligible, hovering at around %0.5 on
my older machines.

### Future
This is just an experiment, but it could be polished up a little bit more. For
example, it's storing the conversations in a SQLite db, already, with each 
session given a unique ID, so that SQL retrieval can easily fetch the whole 
session. Here are some ideas for improvements:

1. Replace the "Start Recording" and "Stop Recording" links with a single button that toggles between the two states and perhaps changes color to indicate its state.
2. Add a "Clear Conversation" link to the menu that will regenerate the session ID.
3. Add a link to the menu that will open a window with the conversation history.
4. Add a link to the menu that will open a window with the settings.
5. Add a link to the menu that will open a file dialog, allowing the user to select one or more documents to chat with the language model about (RAG).
6. Add a link that opens a window with the available models from the specified Ollama server, and allows the user to select a different model.
7. Of course, if the user selects the multimodal model, the user should be able to select one or more images to chat about.